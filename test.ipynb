{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datamodule import CT_Datamodule\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = CT_Datamodule(\"Dataset\", num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.prepare_data()\n",
    "d.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(loader: torch.utils.data.DataLoader):\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard deviation of the data in the loader\n",
    "\n",
    "    Args:\n",
    "        loader (torch.utils.data.DataLoader): The data loader\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor]: The channel-wise mean and standard deviation of the data\n",
    "    \"\"\"\n",
    "    psum = torch.tensor([0.0] * loader.dataset[0][2][0].shape[0])\n",
    "    psum_sq = torch.tensor([0.0] * loader.dataset[0][2][0].shape[0])\n",
    "\n",
    "    for _, _, inputs, _ in loader:\n",
    "        inputs = inputs[0]\n",
    "        psum += inputs.sum(axis=[0, 2, 3])\n",
    "        psum_sq += (inputs**2).sum(axis=[0, 2, 3])\n",
    "\n",
    "    count = (\n",
    "        len(loader.dataset)\n",
    "        * loader.dataset[0][2][0].shape[1]\n",
    "        * loader.dataset[0][2][0].shape[2]\n",
    "    )\n",
    "\n",
    "    total_mean = psum / count\n",
    "    total_var = (psum_sq / count) - (total_mean**2)\n",
    "    total_std = torch.sqrt(total_var)\n",
    "    return total_mean, total_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1737, 0.1737, 0.1737]), tensor([0.2584, 0.2584, 0.2584]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_stats(d.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "samples_reg = torch.zeros(4)\n",
    "samples_tum = torch.zeros(4)\n",
    "\n",
    "for i, j, _, _ in d.train:\n",
    "    samples_reg += i\n",
    "    samples_tum += j\n",
    "\n",
    "for i, j, _, _ in d.val:\n",
    "    samples_reg += i\n",
    "    samples_tum += j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 99., 362., 114.,  75.], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_tum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([305., 201.,  80.,  64.], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Owais Makroo/.cache\\torch\\hub\\facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "m= torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NestedTensorBlock(\n",
       "  (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (attn): MemEffAttention(\n",
       "    (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (ls1): LayerScale()\n",
       "  (drop_path1): Identity()\n",
       "  (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (mlp): Mlp(\n",
       "    (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "    (act): GELU(approximate='none')\n",
       "    (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (ls2): LayerScale()\n",
       "  (drop_path2): Identity()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Owais Makroo/.cache\\torch\\hub\\facebookresearch_dinov2_main\n",
      "C:\\Users\\Owais Makroo/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "C:\\Users\\Owais Makroo/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "C:\\Users\\Owais Makroo/.cache\\torch\\hub\\facebookresearch_dinov2_main\\dinov2\\layers\\block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "m = Classifier(1e-3, 2e-5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (conLoss): SupervisedContrastiveLoss()\n",
       "  (ceLoss_ppgl): CrossEntropyLoss()\n",
       "  (ceLoss_region): CrossEntropyLoss()\n",
       "  (multiclass_accuracy): MulticlassAccuracy()\n",
       "  (total_accuracy): MulticlassAccuracy()\n",
       "  (multiclass_f1): MulticlassF1Score()\n",
       "  (total_f1): MulticlassF1Score()\n",
       "  (multiclass_auc): MulticlassAUROC()\n",
       "  (total_auc): MulticlassAUROC()\n",
       "  (multiclass_rec): MulticlassRecall()\n",
       "  (total_rec): MulticlassRecall()\n",
       "  (vit): ViTFeatureExtractor(\n",
       "    (base_model): DinoVisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 384, kernel_size=(14, 14), stride=(14, 14))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x NestedTensorBlock(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MemEffAttention(\n",
       "            (qkv): Module(\n",
       "              (original): Linear(in_features=384, out_features=1152, bias=True)\n",
       "              (lora): LoRALayer(\n",
       "                (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "                (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Module(\n",
       "              (original): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (lora): LoRALayer(\n",
       "                (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "                (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls1): LayerScale()\n",
       "          (drop_path1): Identity()\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Module(\n",
       "              (original): Linear(in_features=384, out_features=1536, bias=True)\n",
       "              (lora): LoRALayer(\n",
       "                (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "                (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Module(\n",
       "              (original): Linear(in_features=1536, out_features=384, bias=True)\n",
       "              (lora): LoRALayer(\n",
       "                (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "                (dropout): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ls2): LayerScale()\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (adapters): ModuleDict(\n",
       "      (blocks_0_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_0_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_0_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_0_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_1_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_1_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_1_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_1_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_2_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_2_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_2_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_2_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_3_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_3_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_3_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_3_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_4_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_4_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_4_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_4_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_5_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_5_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_5_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_5_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_6_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_6_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_6_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_6_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_7_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_7_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_7_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_7_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_8_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_8_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_8_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_8_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_9_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_9_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_9_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_9_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_10_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_10_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_10_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_10_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_11_attn_qkv): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1152, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_11_attn_proj): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_11_mlp_fc1): LoRALayer(\n",
       "        (lora_down): Linear(in_features=384, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=1536, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "      (blocks_11_mlp_fc2): LoRALayer(\n",
       "        (lora_down): Linear(in_features=1536, out_features=16, bias=False)\n",
       "        (lora_up): Linear(in_features=16, out_features=384, bias=False)\n",
       "        (dropout): Dropout(p=0.05, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (feature_extractor): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=256, bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "  )\n",
       "  (classifiers): ModuleList(\n",
       "    (0-3): 4 x Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (softmax): Softmax(dim=-1)\n",
       "  (region_classifier): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fld_fcst",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
